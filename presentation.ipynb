{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey! This notebook has been further analysed in my presentation. If you have any questions about how it works or tips on improving it for further education please let me know at khanmomin2468@gmail.com! \n",
    "\n",
    "As for optimization, though I always welcome criticism of major errors and/or optimization paths, the purpose of this notebook is solely to educate and not total accuracy. \n",
    "\n",
    "Thank you so much, enjoy!\n",
    "\n",
    "The dataset used in this presentation is further explained by the diagnosis.names file, provided graciously by Dr. Jacek Czerniak, Ph.D. at the Polish Academy of Sciences Systems Research Institute. This dataset was specifically chosen due to its relative ease of understanding, small size and real-life connection. Its easier to see the power of machine learning to help people when the data is medical rather than just creating some image recognition program. \n",
    "\n",
    "From here on out, all comments made are in relation to the code. Good luck!\n",
    "\n",
    "The section below is simply for importing libraries. The libraries we will be using today are: \n",
    "\n",
    "    - Tensorflow (https://www.infoworld.com/article/3278008/what-is-tensorflow-the-machine-learning-library-explained.html)\n",
    "\n",
    "    - Numpy (https://numpy.org/doc/stable/user/whatisnumpy.html)\n",
    "\n",
    "    - Pandas (https://www.activestate.com/resources/quick-reads/what-is-pandas-in-python-everything-you-need-to-know/)\n",
    "\n",
    "    - Matplotlib (https://matplotlib.org/)\n",
    "\n",
    "For a more detailed analysis of what each library does I highly suggest you click the links above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from keras.constraints import max_norm\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#used for display purposes\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is one of the most used libraries out there, used for list and array management, matrix data structures, and basically any mathematical function you could possibly need\n",
    "\n",
    "Pandas is a library used for data analysis. Much like numpy, pandas is used for its extremely useful data structures (i.e dataframes). However unlike numpy, pandas is primarily used for data analysis, data structuring and manipulation (i.e merging, reshaping, cleaning and selecting). \n",
    "\n",
    "You can imagine pandas as \"Data representation and morphing\" and numpy as \"Array management and the math work-horse\"\n",
    "\n",
    "Matplotlib is simply there to display any data I would need to show. Graphs. Thats all its for. Graphs. \n",
    "\n",
    "The big boy of the group is Tensorflow: \n",
    "https://www.tensorflow.org/tutorials/keras/classification\n",
    "\n",
    "Above is a tutorial on how to start on your very first machine learning algorithm. Though it does a good job explaining how to do machine learning, I'll be honest I find diagnosing diseases to be more interesting and engaging. Sue me. \n",
    "\n",
    "Tensorflow is a \"end to end open source platform for machine learning\" (source: tensorflow.org)\n",
    "\n",
    "What that means is that from start to end, everything machine learning related is possible through this API. It is highly optimized and very user friendly. I highly recommend anyone starting with machine learning to NOT START HERE!!!\n",
    "\n",
    "Although the allure of \"fast and easy\" is strong, it will slow you down in the long run. Tensorflow ends up doing so much for you its difficult to fully understand the concepts behind much of what is going on. Functions like ReLU, Softmax, forward prop and backwards prop become merely words and requirements rather than fundamental functions to be understood. \n",
    "\n",
    "Going forward however, for the purposes of not scaring people away we will be using tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.abspath(\"data\\diagnosis.csv\")\n",
    "data = pd.read_csv(filename, names= [\"Temp\", \"Nausea\", \"Lumbar\", \"Urine\", \"micturition\", \"burning\", \"decision1\", \"decision2\"])\n",
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that word vomit above, from here on out the comments will be as short as possible, don't worry. \n",
    "\n",
    "Above we are simply pulling the data from a csv (provided), then converting it to an numpy array. This will make it possible to do some much needed linear algebra. Spooky, I know. But trust me, you will find that it is not too much to understand. \n",
    "\n",
    "We then pull the data's shape (column and row), then randomly shuffle the data. One thing to understand is that, much like the graph named \"overfit.png\", the more you train a machine learning algorithm on the exact same data, the more it becomes molded to that set of data. In other words, the algorithm over-fits and ends up creating less accurate results when shown data it has never seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS THE HEAD OF Y_DEV:\n",
      "\n",
      "[3, 3]\n",
      "THIS IS THE HEAD OF X_DEV:\n",
      "\n",
      "[[0.9738095238095238 0.9666666666666667 0.8833333333333334\n",
      "  0.9214285714285715 0.8928571428571429 0.8761904761904761\n",
      "  0.8999999999999999 0.8809523809523809 0.9666666666666667\n",
      "  0.9523809523809523 0.9119047619047619 0.9690476190476192\n",
      "  0.9619047619047618 0.888095238095238 0.8928571428571429\n",
      "  0.9666666666666667 0.888095238095238 0.8809523809523809\n",
      "  0.9761904761904762 0.9595238095238094 0.8976190476190476\n",
      "  0.8547619047619047 0.9523809523809523 0.9761904761904762\n",
      "  0.8738095238095239 0.8857142857142858 0.8833333333333334\n",
      "  0.861904761904762 0.8571428571428571 0.8952380952380953\n",
      "  0.9023809523809524 0.8785714285714286 0.8714285714285714\n",
      "  0.9880952380952381 0.8999999999999999 0.8928571428571429\n",
      "  0.9023809523809524 0.9023809523809524 0.9880952380952381\n",
      "  0.8738095238095239 0.9047619047619048 0.9761904761904762\n",
      "  0.9833333333333333 0.9785714285714286 0.8976190476190476\n",
      "  0.9690476190476192 0.980952380952381 0.9880952380952381\n",
      "  0.8809523809523809 0.8571428571428571 0.9785714285714286\n",
      "  0.8928571428571429 0.8452380952380952 0.9023809523809524\n",
      "  0.9571428571428572 0.8642857142857142 0.9738095238095238\n",
      "  0.980952380952381 0.8571428571428571 0.8761904761904761]\n",
      " [1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "data_dev = data[0:int(m/2)].T\n",
    "Y_dev = data_dev[(n-2):n]\n",
    "X_dev = data_dev[0:(n-2)]\n",
    "\n",
    "for i in range(len(Y_dev)):\n",
    "    x = 0\n",
    "    for y in Y_dev[i]:\n",
    "        \n",
    "        if \"no\" in y:\n",
    "            y = 0\n",
    "        else:\n",
    "            y = 1\n",
    "        Y_dev[i][x] = y\n",
    "        x+=1 \n",
    "Y_labels = []\n",
    "for i in range(len(Y_dev[0])):\n",
    "    temp = [Y_dev[0][i], Y_dev[1][i]]\n",
    "    if temp == [0,0]:\n",
    "        temp = 0\n",
    "    if temp == [0,1]:\n",
    "        temp = 1\n",
    "    if temp == [1,0]:\n",
    "        temp = 2\n",
    "    if temp == [1,1]:\n",
    "        temp = 3\n",
    "    Y_labels.append(temp)\n",
    "Y_dev = Y_labels\n",
    "for i in range(len(X_dev)-1):\n",
    "\n",
    "    for y in range(len(X_dev[i])):\n",
    "        if \"no\" in X_dev[i+1][y]:\n",
    "            temp = 0\n",
    "        else:\n",
    "            temp = 1\n",
    "        X_dev[i+1][y] = temp\n",
    "i=0\n",
    "for x in X_dev[0]:\n",
    "    x = x.replace(\",\" , \".\")  \n",
    "    X_dev[0][i] = float((x))/42\n",
    "    i+=1\n",
    "\n",
    "data_train = data[int(m/2):m].T\n",
    "Y_train = data_train[(n-2):n]\n",
    "X_train = data_train[0:(n-2)]\n",
    "i = 0\n",
    "for i in range(len(Y_train)):\n",
    "    x = 0\n",
    "    for y in Y_train[i]:\n",
    "        \n",
    "        if \"no\" in y:\n",
    "            y = 0\n",
    "        else:\n",
    "            y = 1\n",
    "        Y_train[i][x] = y\n",
    "        x+=1 \n",
    "Y_labels = []\n",
    "for i in range(len(Y_train[0])):\n",
    "    temp = [Y_train[0][i], Y_train[1][i]]\n",
    "    if temp == [0,0]:\n",
    "        temp = 0\n",
    "    if temp == [0,1]:\n",
    "        temp = 1\n",
    "    if temp == [1,0]:\n",
    "        temp = 2\n",
    "    if temp == [1,1]:\n",
    "        temp = 3\n",
    "    Y_labels.append(temp)\n",
    "Y_train = Y_labels\n",
    "for i in range(len(X_train)-1):\n",
    "    \n",
    "    for y in range(len(X_train[i])):\n",
    "        if \"no\" in X_train[i+1][y]:\n",
    "            temp = 0\n",
    "        else:\n",
    "            temp = 1\n",
    "        X_train[i+1][y] = temp\n",
    "i = 0\n",
    "for x in X_train[0]:\n",
    "    try:    \n",
    "        x = x.replace(\",\" , \".\")  \n",
    "    except:\n",
    "        x = x\n",
    "    X_train[0][i] = float((x))/42\n",
    "    i+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"THIS IS THE HEAD OF Y_DEV:\\n\")\n",
    "print(Y_dev[:2])\n",
    "print(\"THIS IS THE HEAD OF X_DEV:\\n\")\n",
    "print(X_dev[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, having taken the data from the csv, we now need to make it useable to the machine learning algorithm. \n",
    "\n",
    "For our purposes, we want everything to be between 1 and 0. In this case, yes's and no's are 1's and 0's, and the temperature is \"normalized\" and made such that all values are between 1 and 0. This is done by dividing all values by the highest value in the array. \n",
    "\n",
    "This makes everything so much easier to do in the future, and is also a common practice in the industry for applicable datasets. \n",
    "\n",
    "On top of this, we have seperated the data into two sections. X and Y. X is all of the data we recieved from the patient, those being: \"Temperature\", \"Nausea\", \"Lumbar Pain\", \"Urine pushing\", \"micturition pains\", \"burning\".\n",
    "\n",
    "Y is all of the results. This is an array of the final diagnosis made by the doctor: was it either \"Inflammation of urinary bladder\", \"Nephritis of renal pelvis origin\", neither or both? \n",
    "\n",
    "We have then further split the data into \"Developmental\" or otherwise known as \"validation\" data, and \"training\" data. \n",
    "\n",
    "To train the machine, we feed it the X_train array. It will then come up with a pseudo-random answer in the very beginning. The error will be very high, calculated by checking if the output of the algorithm matches the real answer (Y_train). Once a sufficient enough amount of cycles is completed of this constant error checking and correction, the algorithm then checks with the validation data (data it has never seen) to make sure that the algorithm outputs the right answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 6)\n",
      "(60, 6)\n",
      "(60, 4)\n",
      "(60, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(\"float32\").T\n",
    "X_dev = X_dev.astype(\"float32\").T\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_dev = np_utils.to_categorical(Y_dev)\n",
    "class_num = Y_dev.shape[1]\n",
    "print(X_dev.shape)\n",
    "print(X_train.shape)\n",
    "print(Y_dev.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a most simple level, the algorithm works under a hidden layer of nodes, all interconnected until there is a layer of output nodes. Each node corresponds to a possible answer, and whicever node has the highest value is the most likely answer. \n",
    "\n",
    "We have also now \"categorized\" the Y arrays (the answers). This makes it so that the output can be understood quickly by simply seeing what the output number is. \n",
    "\n",
    "To_categorical converts an array of unique answers and creates an array much like the one below, with each column representing one possible answer. This is also known as one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "demo = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n",
    "demo = tf.constant(demo, shape=[4, 4])\n",
    "print(demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 32)                224       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 356\n",
      "Trainable params: 356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(tf.keras.Input(shape = (6,)))\n",
    "model.add(keras.layers.Dense(32, activation='relu')) #the 32 stands for the output size of the array\n",
    "model.add(keras.layers.Dense(4,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "          loss='binary_crossentropy',\n",
    "          metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the meat and potatoes. The reason you and I are here. \n",
    "\n",
    "So, from the top. We will be designing a sequential model. This is that typical model you will see depicted as a mesh of neurons (see neurondiagram.jpeg). This means each layer output feeds into the next layer until the output layer is reached. \n",
    "\n",
    "The first layer is the input layer, with an input dimension of 6. This is because for each patient, there is 6 variables we are inputting for every output. (6 columns in X_train)\n",
    "\n",
    "The second layer is just an extra one put in as a \"hidden\" layer. Depending on the nature of your data (i.e the amount, the variance within), having more or less layers is better. Plus, the more layers exist, the more computationally intensive the algorithm will be. For now, we will only have one hidden layer. \n",
    "\n",
    "The activation is a bit complicated, but for now you can take it as how \"bright\" each neuron is based on an input. \n",
    "\n",
    "Finally you compile the model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.6643 - accuracy: 0.3500 - val_loss: 0.6258 - val_accuracy: 0.4667\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6518 - accuracy: 0.3500 - val_loss: 0.6168 - val_accuracy: 0.4667\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6429 - accuracy: 0.3500 - val_loss: 0.6093 - val_accuracy: 0.4667\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6357 - accuracy: 0.3500 - val_loss: 0.6029 - val_accuracy: 0.4667\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6295 - accuracy: 0.3500 - val_loss: 0.5972 - val_accuracy: 0.4667\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6239 - accuracy: 0.3500 - val_loss: 0.5920 - val_accuracy: 0.4667\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6189 - accuracy: 0.3500 - val_loss: 0.5872 - val_accuracy: 0.4667\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6142 - accuracy: 0.3500 - val_loss: 0.5826 - val_accuracy: 0.4667\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6097 - accuracy: 0.3500 - val_loss: 0.5783 - val_accuracy: 0.4667\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6054 - accuracy: 0.3667 - val_loss: 0.5742 - val_accuracy: 0.5500\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6014 - accuracy: 0.4333 - val_loss: 0.5702 - val_accuracy: 0.5500\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5975 - accuracy: 0.4333 - val_loss: 0.5664 - val_accuracy: 0.5500\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5938 - accuracy: 0.4333 - val_loss: 0.5626 - val_accuracy: 0.5500\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5901 - accuracy: 0.4333 - val_loss: 0.5590 - val_accuracy: 0.5500\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5866 - accuracy: 0.4333 - val_loss: 0.5555 - val_accuracy: 0.5500\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5832 - accuracy: 0.4333 - val_loss: 0.5521 - val_accuracy: 0.5500\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5800 - accuracy: 0.4333 - val_loss: 0.5489 - val_accuracy: 0.5500\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5768 - accuracy: 0.4333 - val_loss: 0.5457 - val_accuracy: 0.5500\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5737 - accuracy: 0.4333 - val_loss: 0.5427 - val_accuracy: 0.5500\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5707 - accuracy: 0.4333 - val_loss: 0.5397 - val_accuracy: 0.5500\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5679 - accuracy: 0.4333 - val_loss: 0.5368 - val_accuracy: 0.5500\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5651 - accuracy: 0.4333 - val_loss: 0.5340 - val_accuracy: 0.5500\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5624 - accuracy: 0.4333 - val_loss: 0.5312 - val_accuracy: 0.5500\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5597 - accuracy: 0.4333 - val_loss: 0.5285 - val_accuracy: 0.5500\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5570 - accuracy: 0.4333 - val_loss: 0.5258 - val_accuracy: 0.5500\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5544 - accuracy: 0.4333 - val_loss: 0.5231 - val_accuracy: 0.5500\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5518 - accuracy: 0.4333 - val_loss: 0.5205 - val_accuracy: 0.5500\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5492 - accuracy: 0.4333 - val_loss: 0.5180 - val_accuracy: 0.5500\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5467 - accuracy: 0.4333 - val_loss: 0.5154 - val_accuracy: 0.5500\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5442 - accuracy: 0.4333 - val_loss: 0.5129 - val_accuracy: 0.5500\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5418 - accuracy: 0.4333 - val_loss: 0.5105 - val_accuracy: 0.5500\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5394 - accuracy: 0.4333 - val_loss: 0.5080 - val_accuracy: 0.5500\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5370 - accuracy: 0.4333 - val_loss: 0.5056 - val_accuracy: 0.5500\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5347 - accuracy: 0.4333 - val_loss: 0.5033 - val_accuracy: 0.5500\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5323 - accuracy: 0.4333 - val_loss: 0.5009 - val_accuracy: 0.5500\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5300 - accuracy: 0.4333 - val_loss: 0.4986 - val_accuracy: 0.5500\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5278 - accuracy: 0.4333 - val_loss: 0.4963 - val_accuracy: 0.5500\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5255 - accuracy: 0.4333 - val_loss: 0.4940 - val_accuracy: 0.5500\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5232 - accuracy: 0.4333 - val_loss: 0.4917 - val_accuracy: 0.5500\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5210 - accuracy: 0.4333 - val_loss: 0.4895 - val_accuracy: 0.5500\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5188 - accuracy: 0.4333 - val_loss: 0.4873 - val_accuracy: 0.5500\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5166 - accuracy: 0.4333 - val_loss: 0.4851 - val_accuracy: 0.5500\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5144 - accuracy: 0.4333 - val_loss: 0.4829 - val_accuracy: 0.5500\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5122 - accuracy: 0.4333 - val_loss: 0.4807 - val_accuracy: 0.5500\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5101 - accuracy: 0.4333 - val_loss: 0.4786 - val_accuracy: 0.5500\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5080 - accuracy: 0.4333 - val_loss: 0.4764 - val_accuracy: 0.5500\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5059 - accuracy: 0.4333 - val_loss: 0.4743 - val_accuracy: 0.5500\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5038 - accuracy: 0.4333 - val_loss: 0.4722 - val_accuracy: 0.5500\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5017 - accuracy: 0.4333 - val_loss: 0.4702 - val_accuracy: 0.5500\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4996 - accuracy: 0.4333 - val_loss: 0.4681 - val_accuracy: 0.5500\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4976 - accuracy: 0.4333 - val_loss: 0.4661 - val_accuracy: 0.5500\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4956 - accuracy: 0.4333 - val_loss: 0.4641 - val_accuracy: 0.5500\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4936 - accuracy: 0.4333 - val_loss: 0.4621 - val_accuracy: 0.5500\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4917 - accuracy: 0.4333 - val_loss: 0.4602 - val_accuracy: 0.5500\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4898 - accuracy: 0.4333 - val_loss: 0.4583 - val_accuracy: 0.5500\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4878 - accuracy: 0.4333 - val_loss: 0.4563 - val_accuracy: 0.5500\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4859 - accuracy: 0.4333 - val_loss: 0.4543 - val_accuracy: 0.6333\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4840 - accuracy: 0.5167 - val_loss: 0.4524 - val_accuracy: 0.6833\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4820 - accuracy: 0.6500 - val_loss: 0.4505 - val_accuracy: 0.6833\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4801 - accuracy: 0.6500 - val_loss: 0.4486 - val_accuracy: 0.6833\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4783 - accuracy: 0.6500 - val_loss: 0.4467 - val_accuracy: 0.6833\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4764 - accuracy: 0.6500 - val_loss: 0.4449 - val_accuracy: 0.6833\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4746 - accuracy: 0.6500 - val_loss: 0.4430 - val_accuracy: 0.6833\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4727 - accuracy: 0.6500 - val_loss: 0.4412 - val_accuracy: 0.6833\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4709 - accuracy: 0.6500 - val_loss: 0.4393 - val_accuracy: 0.6833\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4691 - accuracy: 0.6500 - val_loss: 0.4375 - val_accuracy: 0.6833\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4672 - accuracy: 0.6500 - val_loss: 0.4356 - val_accuracy: 0.6833\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4654 - accuracy: 0.6500 - val_loss: 0.4337 - val_accuracy: 0.8333\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4635 - accuracy: 0.8333 - val_loss: 0.4319 - val_accuracy: 0.8333\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4617 - accuracy: 0.8333 - val_loss: 0.4301 - val_accuracy: 0.8333\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4599 - accuracy: 0.8333 - val_loss: 0.4284 - val_accuracy: 0.8333\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4581 - accuracy: 0.8333 - val_loss: 0.4266 - val_accuracy: 0.8333\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4563 - accuracy: 0.8333 - val_loss: 0.4248 - val_accuracy: 0.8333\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4545 - accuracy: 0.8333 - val_loss: 0.4231 - val_accuracy: 0.8333\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4528 - accuracy: 0.8333 - val_loss: 0.4213 - val_accuracy: 0.8333\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4510 - accuracy: 0.8333 - val_loss: 0.4196 - val_accuracy: 0.8333\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4493 - accuracy: 0.8333 - val_loss: 0.4178 - val_accuracy: 0.8333\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4476 - accuracy: 0.8333 - val_loss: 0.4161 - val_accuracy: 0.8333\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4458 - accuracy: 0.8333 - val_loss: 0.4143 - val_accuracy: 0.8333\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4441 - accuracy: 0.8333 - val_loss: 0.4127 - val_accuracy: 0.8333\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4424 - accuracy: 0.8333 - val_loss: 0.4109 - val_accuracy: 0.8333\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4407 - accuracy: 0.8333 - val_loss: 0.4093 - val_accuracy: 0.8333\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4390 - accuracy: 0.8333 - val_loss: 0.4075 - val_accuracy: 0.8333\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4373 - accuracy: 0.8333 - val_loss: 0.4060 - val_accuracy: 0.8333\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4356 - accuracy: 0.8333 - val_loss: 0.4042 - val_accuracy: 0.8333\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4339 - accuracy: 0.8333 - val_loss: 0.4027 - val_accuracy: 0.8333\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4323 - accuracy: 0.8333 - val_loss: 0.4011 - val_accuracy: 0.8333\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4306 - accuracy: 0.8333 - val_loss: 0.3994 - val_accuracy: 0.8333\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4289 - accuracy: 0.8333 - val_loss: 0.3978 - val_accuracy: 0.8333\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4272 - accuracy: 0.8333 - val_loss: 0.3962 - val_accuracy: 0.8333\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4256 - accuracy: 0.8333 - val_loss: 0.3946 - val_accuracy: 0.8333\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4240 - accuracy: 0.8333 - val_loss: 0.3930 - val_accuracy: 0.8333\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4223 - accuracy: 0.8333 - val_loss: 0.3914 - val_accuracy: 0.8333\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4207 - accuracy: 0.8333 - val_loss: 0.3899 - val_accuracy: 0.8333\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4191 - accuracy: 0.8333 - val_loss: 0.3883 - val_accuracy: 0.8333\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4174 - accuracy: 0.8333 - val_loss: 0.3868 - val_accuracy: 0.8333\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4158 - accuracy: 0.8667 - val_loss: 0.3853 - val_accuracy: 0.9167\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4142 - accuracy: 0.9167 - val_loss: 0.3838 - val_accuracy: 0.9167\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4126 - accuracy: 0.9167 - val_loss: 0.3823 - val_accuracy: 0.9167\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4110 - accuracy: 0.9167 - val_loss: 0.3808 - val_accuracy: 0.9167\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4094 - accuracy: 0.9167 - val_loss: 0.3793 - val_accuracy: 0.9167\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4079 - accuracy: 0.9167 - val_loss: 0.3778 - val_accuracy: 0.9167\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4063 - accuracy: 0.9167 - val_loss: 0.3763 - val_accuracy: 0.9167\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4047 - accuracy: 0.9167 - val_loss: 0.3749 - val_accuracy: 0.9167\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4031 - accuracy: 0.9167 - val_loss: 0.3734 - val_accuracy: 0.9167\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4016 - accuracy: 0.9167 - val_loss: 0.3720 - val_accuracy: 0.9167\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4000 - accuracy: 0.9167 - val_loss: 0.3705 - val_accuracy: 0.9167\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3985 - accuracy: 0.9167 - val_loss: 0.3691 - val_accuracy: 0.9167\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3969 - accuracy: 0.9167 - val_loss: 0.3677 - val_accuracy: 0.9167\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3953 - accuracy: 0.9167 - val_loss: 0.3662 - val_accuracy: 0.9167\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3938 - accuracy: 0.9167 - val_loss: 0.3648 - val_accuracy: 0.9167\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3923 - accuracy: 0.9167 - val_loss: 0.3634 - val_accuracy: 0.9167\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3907 - accuracy: 0.9167 - val_loss: 0.3620 - val_accuracy: 0.9167\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3892 - accuracy: 0.9167 - val_loss: 0.3605 - val_accuracy: 0.9167\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3876 - accuracy: 0.9167 - val_loss: 0.3591 - val_accuracy: 0.9167\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3860 - accuracy: 0.9167 - val_loss: 0.3577 - val_accuracy: 0.9167\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3845 - accuracy: 0.9167 - val_loss: 0.3563 - val_accuracy: 0.9167\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3829 - accuracy: 0.9167 - val_loss: 0.3549 - val_accuracy: 0.9167\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3814 - accuracy: 0.9167 - val_loss: 0.3535 - val_accuracy: 0.9167\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3798 - accuracy: 0.9167 - val_loss: 0.3522 - val_accuracy: 0.9167\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3783 - accuracy: 0.9167 - val_loss: 0.3507 - val_accuracy: 0.9167\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3768 - accuracy: 0.9167 - val_loss: 0.3494 - val_accuracy: 0.9167\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3753 - accuracy: 0.9167 - val_loss: 0.3481 - val_accuracy: 0.9167\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3737 - accuracy: 0.9167 - val_loss: 0.3466 - val_accuracy: 0.9167\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3722 - accuracy: 0.9167 - val_loss: 0.3453 - val_accuracy: 0.9167\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3707 - accuracy: 0.9167 - val_loss: 0.3438 - val_accuracy: 0.9167\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3692 - accuracy: 0.9167 - val_loss: 0.3426 - val_accuracy: 0.9167\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3677 - accuracy: 0.9167 - val_loss: 0.3410 - val_accuracy: 0.9167\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3662 - accuracy: 0.9167 - val_loss: 0.3397 - val_accuracy: 0.9167\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3647 - accuracy: 0.9167 - val_loss: 0.3384 - val_accuracy: 0.9167\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3632 - accuracy: 0.9167 - val_loss: 0.3369 - val_accuracy: 0.9167\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3617 - accuracy: 0.9167 - val_loss: 0.3356 - val_accuracy: 0.9167\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3602 - accuracy: 0.9167 - val_loss: 0.3342 - val_accuracy: 0.9167\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3587 - accuracy: 0.9167 - val_loss: 0.3328 - val_accuracy: 0.9167\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3572 - accuracy: 0.9167 - val_loss: 0.3316 - val_accuracy: 0.9167\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3557 - accuracy: 0.9167 - val_loss: 0.3301 - val_accuracy: 0.9167\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3542 - accuracy: 0.9167 - val_loss: 0.3288 - val_accuracy: 0.9167\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3528 - accuracy: 0.9167 - val_loss: 0.3274 - val_accuracy: 0.9167\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3513 - accuracy: 0.9167 - val_loss: 0.3262 - val_accuracy: 0.9167\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3499 - accuracy: 0.9167 - val_loss: 0.3246 - val_accuracy: 0.9167\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3484 - accuracy: 0.9167 - val_loss: 0.3234 - val_accuracy: 0.9167\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3469 - accuracy: 0.9167 - val_loss: 0.3220 - val_accuracy: 0.9167\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3455 - accuracy: 0.9167 - val_loss: 0.3207 - val_accuracy: 0.9167\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3441 - accuracy: 0.9167 - val_loss: 0.3194 - val_accuracy: 0.9167\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3426 - accuracy: 0.9167 - val_loss: 0.3181 - val_accuracy: 0.9167\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3412 - accuracy: 0.9167 - val_loss: 0.3168 - val_accuracy: 0.9167\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3398 - accuracy: 0.9167 - val_loss: 0.3154 - val_accuracy: 0.9167\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3383 - accuracy: 0.9167 - val_loss: 0.3141 - val_accuracy: 0.9167\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3369 - accuracy: 0.9167 - val_loss: 0.3128 - val_accuracy: 0.9167\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3355 - accuracy: 0.9167 - val_loss: 0.3115 - val_accuracy: 0.9167\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3341 - accuracy: 0.9167 - val_loss: 0.3102 - val_accuracy: 0.9167\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3326 - accuracy: 0.9167 - val_loss: 0.3089 - val_accuracy: 0.9167\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3312 - accuracy: 0.9167 - val_loss: 0.3076 - val_accuracy: 0.9167\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3298 - accuracy: 0.9167 - val_loss: 0.3062 - val_accuracy: 0.9167\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3284 - accuracy: 0.9167 - val_loss: 0.3050 - val_accuracy: 0.9167\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3270 - accuracy: 0.9167 - val_loss: 0.3037 - val_accuracy: 0.9167\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3255 - accuracy: 0.9167 - val_loss: 0.3024 - val_accuracy: 0.9167\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3241 - accuracy: 0.9167 - val_loss: 0.3011 - val_accuracy: 0.9167\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3227 - accuracy: 0.9167 - val_loss: 0.2999 - val_accuracy: 0.9167\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3213 - accuracy: 0.9167 - val_loss: 0.2985 - val_accuracy: 0.9167\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3199 - accuracy: 0.9167 - val_loss: 0.2973 - val_accuracy: 0.9167\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3185 - accuracy: 0.9167 - val_loss: 0.2961 - val_accuracy: 0.9167\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3172 - accuracy: 0.9167 - val_loss: 0.2947 - val_accuracy: 0.9167\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3158 - accuracy: 0.9167 - val_loss: 0.2936 - val_accuracy: 0.9167\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3144 - accuracy: 0.9167 - val_loss: 0.2923 - val_accuracy: 0.9167\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3130 - accuracy: 0.9167 - val_loss: 0.2911 - val_accuracy: 0.9167\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3117 - accuracy: 0.9167 - val_loss: 0.2897 - val_accuracy: 0.9167\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3103 - accuracy: 0.9167 - val_loss: 0.2885 - val_accuracy: 0.9167\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3089 - accuracy: 0.9167 - val_loss: 0.2873 - val_accuracy: 0.9167\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3076 - accuracy: 0.9167 - val_loss: 0.2860 - val_accuracy: 0.9167\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3062 - accuracy: 0.9167 - val_loss: 0.2848 - val_accuracy: 0.9167\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3049 - accuracy: 0.9167 - val_loss: 0.2834 - val_accuracy: 0.9167\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3035 - accuracy: 0.9167 - val_loss: 0.2823 - val_accuracy: 0.9167\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3022 - accuracy: 0.9167 - val_loss: 0.2811 - val_accuracy: 0.9167\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3009 - accuracy: 0.9167 - val_loss: 0.2798 - val_accuracy: 0.9167\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2995 - accuracy: 0.9167 - val_loss: 0.2785 - val_accuracy: 0.9167\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2982 - accuracy: 0.9167 - val_loss: 0.2774 - val_accuracy: 0.9167\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2969 - accuracy: 0.9167 - val_loss: 0.2762 - val_accuracy: 0.9167\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2955 - accuracy: 0.9167 - val_loss: 0.2749 - val_accuracy: 0.9167\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2942 - accuracy: 0.9167 - val_loss: 0.2737 - val_accuracy: 0.9167\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2929 - accuracy: 0.9167 - val_loss: 0.2725 - val_accuracy: 0.9167\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2916 - accuracy: 0.9167 - val_loss: 0.2713 - val_accuracy: 0.9167\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2903 - accuracy: 0.9167 - val_loss: 0.2701 - val_accuracy: 0.9167\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2889 - accuracy: 0.9167 - val_loss: 0.2688 - val_accuracy: 0.9167\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2876 - accuracy: 0.9167 - val_loss: 0.2676 - val_accuracy: 0.9167\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2862 - accuracy: 0.9167 - val_loss: 0.2664 - val_accuracy: 0.9167\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2849 - accuracy: 0.9167 - val_loss: 0.2651 - val_accuracy: 0.9167\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2836 - accuracy: 0.9167 - val_loss: 0.2639 - val_accuracy: 0.9167\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2823 - accuracy: 0.9167 - val_loss: 0.2627 - val_accuracy: 0.9167\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2810 - accuracy: 0.9167 - val_loss: 0.2614 - val_accuracy: 0.9167\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2797 - accuracy: 0.9167 - val_loss: 0.2602 - val_accuracy: 0.9167\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2783 - accuracy: 0.9167 - val_loss: 0.2590 - val_accuracy: 0.9167\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2771 - accuracy: 0.9167 - val_loss: 0.2577 - val_accuracy: 0.9167\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2757 - accuracy: 0.9167 - val_loss: 0.2567 - val_accuracy: 0.9167\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2745 - accuracy: 0.9167 - val_loss: 0.2553 - val_accuracy: 0.9167\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2732 - accuracy: 0.9167 - val_loss: 0.2542 - val_accuracy: 0.9167\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2719 - accuracy: 0.9167 - val_loss: 0.2529 - val_accuracy: 0.9167\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2706 - accuracy: 0.9167 - val_loss: 0.2518 - val_accuracy: 0.9167\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2694 - accuracy: 0.9167 - val_loss: 0.2506 - val_accuracy: 0.9167\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2681 - accuracy: 0.9167 - val_loss: 0.2494 - val_accuracy: 0.9167\n",
      "Accuracy: 91.67%\n",
      "{'loss': [0.6642882227897644, 0.6517952084541321, 0.6429396867752075, 0.6357171535491943, 0.6294901371002197, 0.6239467859268188, 0.6189037561416626, 0.614217221736908, 0.6097040772438049, 0.6054261326789856, 0.6013649106025696, 0.597481906414032, 0.5937540531158447, 0.5901203751564026, 0.5866279602050781, 0.5832487344741821, 0.5799688100814819, 0.5767810940742493, 0.5736864805221558, 0.5707376003265381, 0.5678918361663818, 0.5650997161865234, 0.5623569488525391, 0.559655487537384, 0.5569983720779419, 0.5543805956840515, 0.5517944097518921, 0.5492448806762695, 0.5467246174812317, 0.544232189655304, 0.5417710542678833, 0.5393680334091187, 0.5370015501976013, 0.5346587300300598, 0.5323423147201538, 0.5300465822219849, 0.5277537703514099, 0.5254729986190796, 0.523218035697937, 0.5209789872169495, 0.5187627077102661, 0.5165764093399048, 0.5144075155258179, 0.5122439861297607, 0.5100973844528198, 0.5079778432846069, 0.505868136882782, 0.5037760734558105, 0.5016987919807434, 0.4996490478515625, 0.4976179301738739, 0.4956127405166626, 0.49362900853157043, 0.49168452620506287, 0.48975247144699097, 0.4878428876399994, 0.4859120547771454, 0.4839647710323334, 0.4820404648780823, 0.4801425039768219, 0.4782662093639374, 0.47640934586524963, 0.47457700967788696, 0.4727368652820587, 0.4709194004535675, 0.46910178661346436, 0.4672253429889679, 0.4653555154800415, 0.46351590752601624, 0.4616958498954773, 0.45989423990249634, 0.4581037759780884, 0.45631518959999084, 0.45454615354537964, 0.4527851641178131, 0.45103123784065247, 0.449294775724411, 0.44755032658576965, 0.44582483172416687, 0.4440935552120209, 0.44241294264793396, 0.4406852722167969, 0.438967764377594, 0.43728163838386536, 0.43560510873794556, 0.43392500281333923, 0.43226537108421326, 0.43059661984443665, 0.42892348766326904, 0.42724525928497314, 0.42560234665870667, 0.4239582121372223, 0.42232635617256165, 0.42068594694137573, 0.4190675914287567, 0.4174419045448303, 0.41583970189094543, 0.41422608494758606, 0.4126335084438324, 0.41102612018585205, 0.40943801403045654, 0.4078513979911804, 0.4062737226486206, 0.4046870470046997, 0.40312302112579346, 0.40157923102378845, 0.40002205967903137, 0.39845579862594604, 0.39690640568733215, 0.3953479528427124, 0.39381903409957886, 0.39226773381233215, 0.39070025086402893, 0.38915035128593445, 0.3875632882118225, 0.3860001266002655, 0.3844551146030426, 0.3829196095466614, 0.38136741518974304, 0.3798193633556366, 0.37830016016960144, 0.3767828047275543, 0.3752554953098297, 0.37374356389045715, 0.37224388122558594, 0.3707354962825775, 0.3692101240158081, 0.36772727966308594, 0.3661976456642151, 0.36468881368637085, 0.363203227519989, 0.3617057502269745, 0.36018282175064087, 0.35869598388671875, 0.3572258949279785, 0.35574495792388916, 0.3542390763759613, 0.35278379917144775, 0.3513146936893463, 0.34985628724098206, 0.34841594099998474, 0.34693536162376404, 0.3454778790473938, 0.34405484795570374, 0.3425983786582947, 0.34118345379829407, 0.3397647738456726, 0.3383220136165619, 0.33690470457077026, 0.33548545837402344, 0.3340546786785126, 0.3326285183429718, 0.33121904730796814, 0.32977408170700073, 0.3283616304397583, 0.3269667327404022, 0.325528085231781, 0.3241233825683594, 0.3227193057537079, 0.3213117718696594, 0.3199116885662079, 0.31854233145713806, 0.3171669542789459, 0.31575679779052734, 0.3143922984600067, 0.3130333125591278, 0.311663419008255, 0.31028422713279724, 0.3089364469051361, 0.30758577585220337, 0.30622920393943787, 0.30488327145576477, 0.3035441040992737, 0.3021870255470276, 0.3008768856525421, 0.2995285093784332, 0.298205703496933, 0.29687222838401794, 0.2955431640148163, 0.2942248284816742, 0.29292169213294983, 0.2916015684604645, 0.29027459025382996, 0.2889268100261688, 0.2875908315181732, 0.2862289547920227, 0.28492432832717896, 0.28359559178352356, 0.28225937485694885, 0.28095027804374695, 0.2796613574028015, 0.2783440053462982, 0.2770528495311737, 0.27573955059051514, 0.2744728922843933, 0.27319440245628357, 0.2719138264656067, 0.2706359922885895, 0.2693726718425751, 0.2681238353252411], 'accuracy': [0.3499999940395355, 0.3499999940395355, 0.3499999940395355, 0.3499999940395355, 0.3499999940395355, 0.3499999940395355, 0.3499999940395355, 0.3499999940395355, 0.3499999940395355, 0.36666667461395264, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.4333333373069763, 0.5166666507720947, 0.6499999761581421, 0.6499999761581421, 0.6499999761581421, 0.6499999761581421, 0.6499999761581421, 0.6499999761581421, 0.6499999761581421, 0.6499999761581421, 0.6499999761581421, 0.6499999761581421, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8666666746139526, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816], 'val_loss': [0.625845730304718, 0.6167595982551575, 0.609300434589386, 0.6028836965560913, 0.5971889495849609, 0.5920184850692749, 0.5872135162353516, 0.5826228857040405, 0.5782792568206787, 0.5741565227508545, 0.5702069401741028, 0.5663782954216003, 0.5625869035720825, 0.5589771270751953, 0.5554896593093872, 0.552132785320282, 0.5488728284835815, 0.5457112789154053, 0.5427023768424988, 0.5397327542304993, 0.5368365049362183, 0.5339965224266052, 0.53121018409729, 0.5284768342971802, 0.5257894992828369, 0.5231390595436096, 0.5205305218696594, 0.5179579257965088, 0.5154142379760742, 0.5129079222679138, 0.5104519724845886, 0.5080333948135376, 0.505646288394928, 0.5032846331596375, 0.500940203666687, 0.49860331416130066, 0.4962829351425171, 0.49399444460868835, 0.49172669649124146, 0.48948293924331665, 0.4872748553752899, 0.48507174849510193, 0.48288044333457947, 0.48070356249809265, 0.4785638749599457, 0.4764336049556732, 0.47433140873908997, 0.4722447097301483, 0.4701842665672302, 0.46813496947288513, 0.4661225974559784, 0.46408557891845703, 0.46214014291763306, 0.46019500494003296, 0.4582799971103668, 0.45634111762046814, 0.45433712005615234, 0.4524249732494354, 0.45048028230667114, 0.4486004412174225, 0.4467029571533203, 0.4448741674423218, 0.44298940896987915, 0.4411691725254059, 0.43928834795951843, 0.43750470876693726, 0.4355727434158325, 0.43374496698379517, 0.4319261908531189, 0.43011656403541565, 0.428373783826828, 0.4265819489955902, 0.42476922273635864, 0.42310720682144165, 0.421315997838974, 0.4195925295352936, 0.41783076524734497, 0.41613656282424927, 0.41433200240135193, 0.4127494692802429, 0.4109059274196625, 0.4092635214328766, 0.4075484573841095, 0.4059775471687317, 0.4042469561100006, 0.40267544984817505, 0.40105170011520386, 0.39940598607063293, 0.3977509140968323, 0.3961590826511383, 0.3945862352848053, 0.39303824305534363, 0.39142289757728577, 0.3898749053478241, 0.38830819725990295, 0.3868243396282196, 0.38527220487594604, 0.3838110566139221, 0.38226616382598877, 0.38076478242874146, 0.37931957840919495, 0.37784528732299805, 0.3763461410999298, 0.37491995096206665, 0.37344247102737427, 0.3719636797904968, 0.3705339729785919, 0.3691478669643402, 0.367675244808197, 0.36624541878700256, 0.3648228347301483, 0.3634318709373474, 0.3619602620601654, 0.36053502559661865, 0.35912665724754333, 0.35771992802619934, 0.3563266694545746, 0.35493919253349304, 0.35348978638648987, 0.3521784543991089, 0.35074958205223083, 0.34936973452568054, 0.34809619188308716, 0.34663042426109314, 0.3452956974506378, 0.3438304662704468, 0.34258225560188293, 0.3410314619541168, 0.33969846367836, 0.33837559819221497, 0.33693400025367737, 0.33558788895606995, 0.33422964811325073, 0.33279767632484436, 0.3315587639808655, 0.33009809255599976, 0.3287722170352936, 0.3273555040359497, 0.32615166902542114, 0.3246172368526459, 0.3234042525291443, 0.32198333740234375, 0.3207300901412964, 0.3193919062614441, 0.3180864453315735, 0.3167622983455658, 0.31543439626693726, 0.3140951693058014, 0.3128398060798645, 0.31147536635398865, 0.31018364429473877, 0.3089137375354767, 0.3075605630874634, 0.30623677372932434, 0.3049983084201813, 0.30370065569877625, 0.30244722962379456, 0.3011249005794525, 0.2999059557914734, 0.2984878420829773, 0.2972799241542816, 0.2961226999759674, 0.294734925031662, 0.2935566008090973, 0.29225099086761475, 0.2910664677619934, 0.28971925377845764, 0.2884896397590637, 0.28728678822517395, 0.2860172390937805, 0.28479593992233276, 0.2834252119064331, 0.28234514594078064, 0.28108641505241394, 0.2798224091529846, 0.27853038907051086, 0.27742525935173035, 0.2761633098125458, 0.2749340832233429, 0.2737071216106415, 0.2725280225276947, 0.27131006121635437, 0.27007806301116943, 0.2687879800796509, 0.26756730675697327, 0.26637810468673706, 0.2650696635246277, 0.2638908624649048, 0.26265645027160645, 0.26139456033706665, 0.26016926765441895, 0.2589934468269348, 0.2577236592769623, 0.2566671371459961, 0.25531870126724243, 0.2542474865913391, 0.25292107462882996, 0.2518268823623657, 0.2505854070186615, 0.24940821528434753], 'val_accuracy': [0.46666666865348816, 0.46666666865348816, 0.46666666865348816, 0.46666666865348816, 0.46666666865348816, 0.46666666865348816, 0.46666666865348816, 0.46666666865348816, 0.46666666865348816, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.550000011920929, 0.6333333253860474, 0.6833333373069763, 0.6833333373069763, 0.6833333373069763, 0.6833333373069763, 0.6833333373069763, 0.6833333373069763, 0.6833333373069763, 0.6833333373069763, 0.6833333373069763, 0.6833333373069763, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.8333333134651184, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816, 0.9166666865348816]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_dev, Y_dev), epochs=200, batch_size=64)\n",
    "\"\"\"Problem with this method is that the testing data is the same as the validation data. \n",
    "Prefferably you would want them to be from seperate datasets. Here we only have test data\"\"\"\n",
    "scores = model.evaluate(X_dev, Y_dev, verbose=0)\n",
    "#Here the accuracy is printed to 2 digits of accuracy\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(history.history)\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.savefig(\"errorplot.png\")\n",
    "plt.show()\n",
    "history.history[\"loss\"][0] = 1.99\n",
    "#print(history.history[\"loss\"][0])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost done here, folks. Now that we have created a model, we must now \"fit\" it to the data. This is where the computationally intensive part comes in. We must now feed the training data into the model, and the validation data to make sure the data is being fit properly. Finally, we evaluate the model based on its accuracy and plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95238096 1.         1.         1.         1.         1.        ]\n",
      " [0.87142855 0.         1.         0.         0.         0.        ]\n",
      " [0.9714286  0.         1.         1.         0.         1.        ]]\n",
      "WARNING:tensorflow:6 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020BF36E03A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[0.02137163 0.24182674 0.10011461 0.53519154]\n",
      " [0.6999373  0.4091652  0.04615679 0.24737057]\n",
      " [0.08886337 0.5927313  0.12448016 0.10073996]]\n",
      "[[0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:3])\n",
    "print(model.predict(X_train[:3]))\n",
    "print(Y_train[:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can now test by hand if the algorithm works. We do this using the predict function. We hand pick some examples (in this case, the first 3 patients), predict using the algorithm and then see if it was correct!\n",
    "\n",
    "Check which of the numbers in the array are the biggest. Those are the answers. They won't all be correct (for, like, a lot of reasons. This is not optimized im making this at 2:30 in the morning timeiseludingmeaaaaaaaaah)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70ae4b4f49f65753036fd11403558d387ac48488314d3a7071c019d9a7af5115"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
